{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook containing the repository's code to run on Kaggle/Colab to use their GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/edomerli/Flatland-3-MARL.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r /kaggle/working/Flatland-3-MARL/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below are needed to solve an issue regarding cuda (https://github.com/pytorch/pytorch/issues/111469)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip uninstall torch torchvision torchaudio -y\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "!mv /kaggle/working/Flatland-3-MARL/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install wandb[media]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from torch import nn\n",
    "import torch\n",
    "import os\n",
    "import pathlib\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import json\n",
    "import yappi\n",
    "\n",
    "from flatland.envs.rail_env import RailEnv\n",
    "from flatland.envs.rail_generators import sparse_rail_generator\n",
    "from flatland.envs.malfunction_generators import ParamMalfunctionGen, MalfunctionParameters\n",
    "from flatland.envs.rail_generators import sparse_rail_generator\n",
    "from flatland.envs.line_generators import sparse_line_generator\n",
    "\n",
    "\n",
    "from utils.render import render_env\n",
    "from utils.seeding import seed_everything\n",
    "# from utils.persister import load_env_from_pickle\n",
    "# from utils.logger import WandbLogger\n",
    "from utils.recorder import RecorderWrapper\n",
    "from utils.env_creator import create_train_env\n",
    "from network.rail_tranformer import RailTranformer\n",
    "from network.mlp import MLP\n",
    "# from reinforcement_learning.ppo import PPO\n",
    "from reinforcement_learning.actor_critic import ActorCritic\n",
    "from env_wrapper.railenv_wrapper import RailEnvWrapper\n",
    "from observation.fast_tree_obs import FastTreeObs\n",
    "\n",
    "# from stable_baselines3 import PPO\n",
    "from reinforcement_learning.ppo import PPO\n",
    "from env_wrapper.skip_no_choice_wrapper import SkipNoChoiceWrapper\n",
    "\n",
    "\n",
    "\n",
    "args = {\n",
    "    \"env_size\": \"demo\",\n",
    "    \"network_architecture\": \"MLP\",\n",
    "    \"skip_no_choice_cells\": False,\n",
    "    \"normalize_v_targets\": False,\n",
    "    \"log_video\": False, # TRY ON LATER\n",
    "}\n",
    "args = Namespace(**args)\n",
    "\n",
    "### OBSERVATION ###\n",
    "TREE_OBS_DEPTH = 2\n",
    "obs_builder = FastTreeObs(max_depth=TREE_OBS_DEPTH)\n",
    "\n",
    "### CONFIGURATION ###\n",
    "TOT_TIMESTEPS = 2**20    # approx 1M\n",
    "ITER_TIMESTEPS = 2**10    # approx 1K\n",
    "NUM_ITERATIONS = TOT_TIMESTEPS // ITER_TIMESTEPS\n",
    "\n",
    "CONFIG = {\n",
    "    # Environment\n",
    "    \"env_size\": args.env_size,\n",
    "    \"skip_no_choice_cells\": args.skip_no_choice_cells,\n",
    "\n",
    "    # Observation\n",
    "    \"tree_obs_depth\": TREE_OBS_DEPTH,\n",
    "\n",
    "    # Timesteps and iterations\n",
    "    \"tot_timesteps\": TOT_TIMESTEPS,\n",
    "    \"iteration_timesteps\": ITER_TIMESTEPS,\n",
    "    \"num_iterations\": NUM_ITERATIONS,\n",
    "\n",
    "    # Network architecture\n",
    "    \"network_architecture\": args.network_architecture,\n",
    "    \"state_size\": obs_builder.observation_dim,\n",
    "    \"action_size\": 4,   # we choose to ignore the \"DO_NOTHING\" action (since semantically superfluous), and work only with \"MOVE_LEFT\", \"MOVE_FORWARD\", \"MOVE_RIGHT\" and \"STOP\"\n",
    "    \"hidden_size\": 128,\n",
    "    \"num_layers\": 3,\n",
    "\n",
    "    # Training params\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 128,\n",
    "    \"learning_rate\": 2.5e-4,\n",
    "    \"kl_limit\": 0.02,\n",
    "    \"adam_eps\": 1e-5,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\n",
    "    # PPO params\n",
    "    \"gamma\": 0.999,\n",
    "    \"lambda_\": 0.95,\n",
    "    \"eps_clip\": 0.2,\n",
    "    \"entropy_bonus\": 1e-2,\n",
    "    \"v_target\": \"TD-lambda\",  # \"TD-lambda\" (for advantage + value) or \"MC\" (for cumulative reward)\n",
    "    \"normalize_v_targets\": args.normalize_v_targets,\n",
    "\n",
    "    # Logging\n",
    "    \"batch_log_frequency\": 10,    # how often to log batch stats\n",
    "    \"log_video\": args.log_video,\n",
    "    \"episode_video_frequency\": 10,\n",
    "\n",
    "    # Wandb\n",
    "    \"wandb\": True,\n",
    "\n",
    "    # Yappi profiling\n",
    "    \"profiling\": False,\n",
    "}\n",
    "\n",
    "### ENVIRONMENT ###\n",
    "env = create_train_env(CONFIG[\"env_size\"])\n",
    "env.obs_builder = obs_builder\n",
    "env.obs_builder.set_env(env)\n",
    "\n",
    "# set random seed in the config\n",
    "CONFIG[\"seed\"] = env.random_seed\n",
    "\n",
    "config = Namespace(**CONFIG)\n",
    "\n",
    "### WANDB ###\n",
    "if config.wandb:\n",
    "    wandb.login(key=\"14a7d0e7554bbddd13ca1a8d45472f7a95e73ca4\")\n",
    "    wandb.init(project=\"flatland-marl\", name=f\"{config.env_size}_{env.number_of_agents}\", config=CONFIG, sync_tensorboard=True)\n",
    "\n",
    "    wandb.define_metric(\"play/step\")\n",
    "    wandb.define_metric(\"play/true_episodic_reward\", step_metric=\"play/step\")\n",
    "    wandb.define_metric(\"play/custom_episodic_reward\", step_metric=\"play/step\")\n",
    "    wandb.define_metric(\"play/percentage_done\", step_metric=\"play/step\")\n",
    "    wandb.define_metric(\"play/episode_length\", step_metric=\"play/step\")\n",
    "\n",
    "    wandb.define_metric(\"train/batch\")\n",
    "    wandb.define_metric(\"train/loss_pi\", step_metric=\"train/batch\")\n",
    "    wandb.define_metric(\"train/loss_v\", step_metric=\"train/batch\")\n",
    "    wandb.define_metric(\"train/entropy\", step_metric=\"train/batch\")\n",
    "    wandb.define_metric(\"train/learning_rate\", step_metric=\"train/batch\")\n",
    "    wandb.define_metric(\"train/kl_div\", step_metric=\"train/batch\")\n",
    "\n",
    "    wandb.define_metric(\"action/step\")\n",
    "    wandb.define_metric(\"action/masked_agent\", step_metric=\"action/step\")\n",
    "    wandb.define_metric(\"action/left\", step_metric=\"action/step\")\n",
    "    wandb.define_metric(\"action/forward\", step_metric=\"action/step\")\n",
    "    wandb.define_metric(\"action/right\", step_metric=\"action/step\")\n",
    "    wandb.define_metric(\"action/stop\", step_metric=\"action/step\")\n",
    "\n",
    "    wandb.define_metric(\"timer/step\")\n",
    "    wandb.define_metric(\"timer/inference\", step_metric=\"timer/step\")\n",
    "    wandb.define_metric(\"timer/env_step\", step_metric=\"timer/step\")\n",
    "    wandb.define_metric(\"timer/reward\", step_metric=\"timer/step\")\n",
    "    wandb.define_metric(\"timer/collection\", step_metric=\"timer/step\")\n",
    "    wandb.define_metric(\"timer/train\", step_metric=\"timer/step\")\n",
    "\n",
    "seed_everything(config.seed)\n",
    "\n",
    "### WRAPPERS ###\n",
    "env = RailEnvWrapper(env) # IMPORTANT: env must be wrapped in RailEnvWrapper before any other wrapper\n",
    "env.obs_builder.set_deadlock_checker(env.deadlock_checker)\n",
    "\n",
    "if config.skip_no_choice_cells:\n",
    "    env = SkipNoChoiceWrapper(env)\n",
    "\n",
    "if config.log_video:\n",
    "    env = RecorderWrapper(env, config.episode_video_frequency)\n",
    "\n",
    "### NETWORK ###\n",
    "if config.network_architecture == \"MLP\":\n",
    "    policy_network = MLP(config.state_size, config.action_size, config.hidden_size, config.num_layers)  # TODO: write that ReLU performs MUCH worse here\n",
    "    value_network = MLP(config.state_size, 1, config.hidden_size, config.num_layers)\n",
    "elif config.network_architecture == \"RailTransformer\":\n",
    "    policy_network = RailTranformer(config.state_size, config.action_size, config.hidden_size, config.num_layers, activation=nn.ReLU)\n",
    "    value_network = RailTranformer(config.state_size, 1, config.hidden_size, config.num_layers, activation=nn.ReLU)\n",
    "    # TODO: voglio provare sia con Tanh che con ReLU, sono troppo curiosooo\n",
    "else:\n",
    "    raise ValueError(\"Invalid network architecture. Must be one of [MLP, RailTransformer]\")\n",
    "\n",
    "### MODEL ###\n",
    "actor_critic = ActorCritic(policy_network, value_network, config)\n",
    "\n",
    "print(f\"Device: {config.device}\")\n",
    "actor_critic.to(config.device)\n",
    "\n",
    "optimizer = torch.optim.Adam(actor_critic.parameters(), lr=config.learning_rate, eps=config.adam_eps)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.num_iterations*config.epochs, eta_min=1e-6)\n",
    "\n",
    "### TRAINING ###\n",
    "ppo = PPO(actor_critic, env, config, optimizer, scheduler)\n",
    "\n",
    "ppo.learn()\n",
    "\n",
    "### SAVE WEIGHTS AND CONFIG ###\n",
    "if not os.path.exists(\"weights\"):\n",
    "    os.makedirs(\"weights\")\n",
    "\n",
    "now = datetime.today().strftime('%Y%m%d-%H%M')\n",
    "weights_path = f\"weights/{now}_policy_{config.network_architecture}_{config.env_size}_{env.number_of_agents}_steps{config.tot_timesteps}_seed{config.seed}.pt\"\n",
    "ppo.save(weights_path)\n",
    "print(f\"Weights saved successfully at {weights_path}!\")\n",
    "\n",
    "# save config as a json file\n",
    "config_path = f\"weights/{now}_config_{config.network_architecture}_{config.env_size}_{env.number_of_agents}_steps{config.tot_timesteps}_seed{config.seed}.json\"\n",
    "CONFIG[\"weights_path\"] = weights_path\n",
    "CONFIG[\"device\"] = config.device.type   # convert device to string for json serialization\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=4)\n",
    "print(f\"Config saved successfully at {config_path}!\")\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flatland-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
