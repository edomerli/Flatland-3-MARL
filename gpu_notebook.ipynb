{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook containing the repository's code to run on Kaggle/Colab to use their GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two middle lines below are needed to solve an issue regarding cuda (https://github.com/pytorch/pytorch/issues/111469)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/edomerli/Flatland-3-MARL.git \n",
    "# !pip install -r /kaggle/working/Flatland-3-MARL/requirements.txt\n",
    "\n",
    "# !pip uninstall torch torchvision torchaudio -y\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# !mv /kaggle/working/Flatland-3-MARL/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "from types import SimpleNamespace\n",
    "from datetime import datetime\n",
    "from torch import nn\n",
    "import torch\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from flatland.envs.rail_env import RailEnv\n",
    "from flatland.envs.rail_generators import sparse_rail_generator\n",
    "from flatland.envs.malfunction_generators import ParamMalfunctionGen, MalfunctionParameters\n",
    "from flatland.envs.rail_generators import sparse_rail_generator\n",
    "from flatland.envs.line_generators import sparse_line_generator\n",
    "\n",
    "\n",
    "from utils.render import render_env\n",
    "from utils.seeding import seed_everything\n",
    "from utils.persister import load_env_from_pickle\n",
    "from utils.logger import WandbLogger\n",
    "from utils.recorder import RecorderWrapper\n",
    "from utils.levels_generator import generate_levels\n",
    "from network.rail_tranformer import RailTranformer\n",
    "# from reinforcement_learning.ppo import PPO\n",
    "from reinforcement_learning.actor_critic import ActorCritic\n",
    "from env_wrapper.railenv_wrapper import RailEnvWrapper\n",
    "from flatland_starter_kit.fast_tree_obs import FastTreeObs\n",
    "# from stable_baselines3 import PPO\n",
    "from reinforcement_learning.ppo import PPO\n",
    "from env_wrapper.skip_no_choice_wrapper import SkipNoChoiceWrapper\n",
    "\n",
    "import yappi\n",
    "\n",
    "\n",
    "### OBSERVATION ###\n",
    "TREE_OBS_DEPTH = 3  # TODO: test with higher\n",
    "obs_builder = FastTreeObs(max_depth=TREE_OBS_DEPTH)\n",
    "\n",
    "### CONFIGURATION ###\n",
    "TOT_TIMESTEPS = 2**18    #2**21  # approx 2M\n",
    "ITER_TIMESTEPS = 2**8    #2**10  # approx 1K\n",
    "NUM_ITERATIONS = TOT_TIMESTEPS // ITER_TIMESTEPS\n",
    "\n",
    "CONFIG = {\n",
    "    # Environment\n",
    "    \"env_size\": \"small\",    # must be one of {\"small\", \"medium\", \"large\", \"huge\"}\n",
    "    # TODO: connect env_size to env pickle file and to loading the respective pretrained model! All automatically\n",
    "    # i.e. without having to specify the filenames etc.\n",
    "    \"skip_no_choice_steps\": False,  # TODO: reintroduci\n",
    "    \"test_id\": \"demo_env\",\n",
    "    \"env_id\": \"Level_1\",\n",
    "\n",
    "    # Observation\n",
    "    \"tree_obs_depth\": TREE_OBS_DEPTH,\n",
    "\n",
    "    # Timesteps and iterations\n",
    "    \"tot_timesteps\": TOT_TIMESTEPS,\n",
    "    \"iteration_timesteps\": ITER_TIMESTEPS,\n",
    "    \"num_iterations\": NUM_ITERATIONS,\n",
    "\n",
    "    # Network architecture\n",
    "    \"model\": \"RailTransformer\",  # \"RailTransformer\" or \"MLP\"   # TODO: implement MLP baseline or remove\n",
    "    \"state_size\": obs_builder.observation_dim,\n",
    "    \"action_size\": 4,\n",
    "    \"hidden_size\": 256,\n",
    "    \"num_layers\": 4,\n",
    "\n",
    "    # Training params\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 128,  # 2**7\n",
    "    \"learning_rate\": 2.5e-4,\n",
    "    \"kl_limit\": 0.02,\n",
    "    \"adam_eps\": 1e-5,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\n",
    "    # PPO params\n",
    "    \"gamma\": 0.999,\n",
    "    \"lambda_\": 0.95,\n",
    "    \"eps_clip\": 0.2,\n",
    "    \"entropy_bonus\": 1e-5,\n",
    "    \"v_target\": \"TD-lambda\",  # \"TD-lambda\" (for advantage + value) or \"MC\" (for cumulative reward)\n",
    "    \"normalize_v_targets\": True,    # TODO: prova con questo OFF\n",
    "\n",
    "    # Logging\n",
    "    \"log_frequency\": 10,\n",
    "    \"log_video\": False,\n",
    "    \"episode_video_frequency\": 10,\n",
    "\n",
    "    # Wandb\n",
    "    \"wandb\": True,\n",
    "\n",
    "    # Yappi profiling\n",
    "    \"profiling\": False,\n",
    "}\n",
    "\n",
    "### ENVIRONMENT ###\n",
    "pickle_train_env_path = f\"./envs_config/train_envs/{CONFIG['test_id']}/{CONFIG['env_id']}.pkl\"\n",
    "\n",
    "# generate the level if the pickle file does not exist\n",
    "if not os.path.exists(pickle_train_env_path):\n",
    "    generate_levels(\"train\", CONFIG[\"test_id\"], CONFIG[\"env_id\"])\n",
    "\n",
    "env = load_env_from_pickle(pickle_train_env_path)\n",
    "\n",
    "env.obs_builder = obs_builder\n",
    "env.obs_builder.set_env(env)\n",
    "\n",
    "# set random seed in the config\n",
    "CONFIG[\"seed\"] = env.random_seed\n",
    "\n",
    "\n",
    "### WANDB ###\n",
    "if CONFIG[\"wandb\"]:\n",
    "    wandb.login(key=\"14a7d0e7554bbddd13ca1a8d45472f7a95e73ca4\")\n",
    "    wandb.init(project=\"flatland-marl\", name=f\"{CONFIG['env_size']}\", config=CONFIG, sync_tensorboard=True)\n",
    "    config = wandb.config\n",
    "\n",
    "    wandb.define_metric(\"play/step\")\n",
    "    wandb.define_metric(\"train/batch\")\n",
    "\n",
    "    wandb.define_metric(\"play/episodic_reward\", step_metric=\"play/step\")\n",
    "    wandb.define_metric(\"play/episode_length\", step_metric=\"play/step\")\n",
    "    wandb.define_metric(\"train/loss_pi\", step_metric=\"train/batch\")\n",
    "    wandb.define_metric(\"train/loss_v\", step_metric=\"train/batch\")\n",
    "    wandb.define_metric(\"train/entropy\", step_metric=\"train/batch\")\n",
    "    wandb.define_metric(\"train/lr_policy\", step_metric=\"train/batch\")\n",
    "    wandb.define_metric(\"train/lr_value\", step_metric=\"train/batch\")\n",
    "    wandb.define_metric(\"test/episodic_reward\", step_metric=\"play/step\")\n",
    "    wandb.define_metric(\"test/episode_length\", step_metric=\"play/step\")\n",
    "else:\n",
    "    config = SimpleNamespace(**CONFIG)\n",
    "\n",
    "seed_everything(config.seed)\n",
    "\n",
    "# IMPORTANT: env must be wrapped in RailEnvWrapper before any other wrapper\n",
    "env = RailEnvWrapper(env)\n",
    "\n",
    "if config.skip_no_choice_steps:\n",
    "    env = SkipNoChoiceWrapper(env)\n",
    "\n",
    "if config.log_video:\n",
    "    env = RecorderWrapper(env, config.episode_video_frequency)\n",
    "\n",
    "\n",
    "# env_steps = 1000  # 2 * env.width * env.height  # Code uses 1.5 to calculate max_steps\n",
    "# rollout_fragment_length = 50\n",
    "# # env = ss.black_death_v2(env)    \n",
    "# env = ss.vector.markov_vector_wrapper.MarkovVectorEnv(env, black_death=True)    # to handle varying number of agents\n",
    "# env = ss.concat_vec_envs_v0(env, 4, num_cpus=1, base_class='stable_baselines3')\n",
    "\n",
    "# env.reset()\n",
    "# o, r, d, i = env.step({i: 0 for i in range(50)})\n",
    "# print(f\"obs: {o}\\n rewards: {r}\\n dones: {d}\\n infos: {i}\")\n",
    "# exit()\n",
    "\n",
    "### NETWORK ###\n",
    "if config.model == \"RailTransformer\":\n",
    "    policy_network = RailTranformer(config.state_size, config.action_size, config.hidden_size, config.num_layers, activation=nn.Tanh)\n",
    "    value_network = RailTranformer(config.state_size, 1, config.hidden_size, config.num_layers, activation=nn.Tanh)\n",
    "    # TODO: voglio provare sia con Tanh che con ReLU, sono troppo curiosooo\n",
    "elif config.model == \"MLP\":\n",
    "    policy_network = nn.Sequential(\n",
    "        nn.Linear(config.state_size, config.hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(config.hidden_size, config.hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(config.hidden_size, config.action_size),\n",
    "        nn.Tanh()\n",
    "    )\n",
    "    value_network = nn.Sequential(\n",
    "        nn.Linear(config.state_size, config.hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(config.hidden_size, config.hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(config.hidden_size, 1)\n",
    "    )\n",
    "\n",
    "### MODEL ###\n",
    "actor_critic = ActorCritic(policy_network, value_network, config)\n",
    "\n",
    "print(f\"Device: {config.device}\")\n",
    "actor_critic.to(config.device)\n",
    "\n",
    "optimizer = torch.optim.Adam(actor_critic.parameters(), lr=config.learning_rate, eps=config.adam_eps)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.num_iterations*config.epochs, eta_min=1e-6)\n",
    "\n",
    "\n",
    "ppo = PPO(actor_critic, env, config, optimizer, scheduler)\n",
    "\n",
    "ppo.learn()\n",
    "\n",
    "now = datetime.today().strftime('%Y%m%d-%H%M')\n",
    "ppo.save(f\"{now}_policy_flatland_{config.env_size}_{config.tot_timesteps}_{config.seed}.pt\")\n",
    "\n",
    "# model = PPO(MlpPolicy, \n",
    "#             env, \n",
    "#             learning_rate=config.lr_policy_network, \n",
    "#             n_steps=config.iteration_timesteps,\n",
    "#             batch_size=config.batch_size, \n",
    "#             n_epochs=config.epochs, \n",
    "#             gamma=config.gamma, \n",
    "#             gae_lambda=config.lambda_,\n",
    "#             clip_range=config.eps_clip, \n",
    "#             normalize_advantage=True, \n",
    "#             ent_coef=config.entropy_bonus,\n",
    "#             # max_grad_norm=0.9, # default=0.5\n",
    "#             verbose=3, \n",
    "#             seed=config.seed)\n",
    "\n",
    "# logger = WandbLogger()\n",
    "# model.set_logger(logger=logger)\n",
    "\n",
    "# TODO: try wandb code below, I think for histograms\n",
    "# wandb.watch(model.policy.action_net, log='all', log_freq = 1)\n",
    "# wandb.watch(model.policy.value_net, log='all', log_freq = 1)\n",
    "# collect rollouts AND train on them\n",
    "\n",
    "# validate performance\n",
    "# TODO: vedi Procgen's test/eval function\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flatland-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
