{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook containing the repository's code to run on Kaggle/Colab to use their GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two middle lines below are needed to solve an issue regarding cuda (https://github.com/pytorch/pytorch/issues/111469)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/edomerli/Flatland-3-MARL.git \n",
    "# !pip install -r /kaggle/working/Flatland-3-MARL/requirements.txt\n",
    "\n",
    "# !pip uninstall torch torchvision torchaudio -y\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# !mv /kaggle/working/Flatland-3-MARL/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmerliedoardo\u001b[0m (\u001b[33mteamedo\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/edo/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/edo/WSL Github repos/Multi-Agent Systems/Flatland-3-MARL/wandb/run-20241211_171845-r1kvh6tq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/teamedo/flatland-marl/runs/r1kvh6tq' target=\"_blank\">demo_5</a></strong> to <a href='https://wandb.ai/teamedo/flatland-marl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/teamedo/flatland-marl' target=\"_blank\">https://wandb.ai/teamedo/flatland-marl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/teamedo/flatland-marl/runs/r1kvh6tq' target=\"_blank\">https://wandb.ai/teamedo/flatland-marl/runs/r1kvh6tq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Device: cpu\n",
      "=======================Iteration: 0=====================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 191\u001b[0m\n\u001b[1;32m    186\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(optimizer, T_max\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_iterations\u001b[38;5;241m*\u001b[39mconfig\u001b[38;5;241m.\u001b[39mepochs, eta_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m    189\u001b[0m ppo \u001b[38;5;241m=\u001b[39m PPO(actor_critic, env, config, optimizer, scheduler)\n\u001b[0;32m--> 191\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m now \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mtoday()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    194\u001b[0m ppo\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_policy_flatland_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mnumber_of_agents\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mtot_timesteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/WSL Github repos/Multi-Agent Systems/Flatland-3-MARL/reinforcement_learning/ppo.py:33\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m     yappi\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# next_obs and next_done are returned such that they can be used as the initial state for the next iteration\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m observations, actions, log_probs, value_targets, advantages, next_obs, next_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_info_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train(observations, actions, log_probs, value_targets, advantages)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mprofiling:\n",
      "File \u001b[0;32m~/WSL Github repos/Multi-Agent Systems/Flatland-3-MARL/reinforcement_learning/ppo.py:128\u001b[0m, in \u001b[0;36mPPO._collect_trajectories\u001b[0;34m(self, next_obs, next_done, initial_info_dict)\u001b[0m\n\u001b[1;32m    119\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplay/true_episodic_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m: normalized_reward,\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplay/custom_episodic_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m: custom_rewards[last_log_step:step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplay/step\u001b[39m\u001b[38;5;124m\"\u001b[39m: global_vars\u001b[38;5;241m.\u001b[39mglobal_step\n\u001b[1;32m    125\u001b[0m })\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# reset the environment\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m next_obs_dict, initial_info_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m next_obs \u001b[38;5;241m=\u001b[39m dict_to_tensor(next_obs_dict)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    130\u001b[0m next_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/WSL Github repos/Multi-Agent Systems/Flatland-3-MARL/env_wrapper/railenv_wrapper.py:48\u001b[0m, in \u001b[0;36mRailEnvWrapper.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 48\u001b[0m     obs, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# cycle until at least one agent is ready to depart\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_required\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/anaconda3/envs/flatland-rl/lib/python3.9/site-packages/flatland/envs/rail_env.py:294\u001b[0m, in \u001b[0;36mRailEnv.reset\u001b[0;34m(self, regenerate_rail, regenerate_schedule, random_seed)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m regenerate_rail \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrail \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrail_generator):\n\u001b[0;32m--> 294\u001b[0m         rail, optionals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrail_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumber_of_agents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_resets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnp_random\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerate\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrail_generator):\n\u001b[1;32m    297\u001b[0m         rail, optionals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrail_generator\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    298\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_of_agents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_resets, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_random)\n",
      "File \u001b[0;32m~/anaconda3/envs/flatland-rl/lib/python3.9/site-packages/flatland/envs/rail_generators.py:43\u001b[0m, in \u001b[0;36mRailGen.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RailGeneratorProduct:\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/flatland-rl/lib/python3.9/site-packages/flatland/envs/rail_generators.py:239\u001b[0m, in \u001b[0;36mSparseRailGen.generate\u001b[0;34m(self, width, height, num_agents, num_resets, np_random)\u001b[0m\n\u001b[1;32m    236\u001b[0m train_stations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trainstation_positions(city_positions, city_radius, free_rails)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Fix all transition elements\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fix_transitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcity_cells\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minter_city_lines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_field\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grid_map, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magents_hints\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity_positions\u001b[39m\u001b[38;5;124m'\u001b[39m: city_positions,\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_stations\u001b[39m\u001b[38;5;124m'\u001b[39m: train_stations,\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity_orientations\u001b[39m\u001b[38;5;124m'\u001b[39m: city_orientations\n\u001b[1;32m    244\u001b[0m }}\n",
      "File \u001b[0;32m~/anaconda3/envs/flatland-rl/lib/python3.9/site-packages/flatland/envs/rail_generators.py:698\u001b[0m, in \u001b[0;36mSparseRailGen._fix_transitions\u001b[0;34m(self, city_cells, inter_city_lines, grid_map, vector_field)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;66;03m# Fix all other cells\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(rails_to_fix_cnt):\n\u001b[0;32m--> 698\u001b[0m     \u001b[43mgrid_map\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfix_transitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrails_to_fix\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrails_to_fix\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrails_to_fix\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/flatland-rl/lib/python3.9/site-packages/flatland/core/transition_map.py:556\u001b[0m, in \u001b[0;36mGridTransitionMap.fix_transitions\u001b[0;34m(self, rcPos, direction)\u001b[0m\n\u001b[1;32m    554\u001b[0m grcMax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# Transition elements\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m transitions \u001b[38;5;241m=\u001b[39m \u001b[43mRailEnvTransitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m cells \u001b[38;5;241m=\u001b[39m transitions\u001b[38;5;241m.\u001b[39mtransition_list\n\u001b[1;32m    558\u001b[0m simple_switch_east_south \u001b[38;5;241m=\u001b[39m transitions\u001b[38;5;241m.\u001b[39mrotate_transition(cells[\u001b[38;5;241m10\u001b[39m], \u001b[38;5;241m90\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/flatland-rl/lib/python3.9/site-packages/flatland/core/grid/rail_env_grid.py:41\u001b[0m, in \u001b[0;36mRailEnvTransitions.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRailEnvTransitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition_list\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# create this to make validation faster\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransitions_all \u001b[38;5;241m=\u001b[39m OrderedSet()\n",
      "File \u001b[0;32m~/anaconda3/envs/flatland-rl/lib/python3.9/site-packages/flatland/core/grid/grid4.py:113\u001b[0m, in \u001b[0;36mGrid4Transitions.__init__\u001b[0;34m(self, transitions)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransitions \u001b[38;5;241m=\u001b[39m transitions\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msDirs \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNESW\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlsDirs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msDirs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# row,col delta for each direction\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgDir2dRC \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mwandb\u001b[0m: ðŸš€ View run \u001b[33mdemo_5\u001b[0m at: \u001b[34mhttps://wandb.ai/teamedo/flatland-marl/runs/r1kvh6tq\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241211_171845-r1kvh6tq/logs\u001b[0m\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "from types import SimpleNamespace\n",
    "from datetime import datetime\n",
    "from torch import nn\n",
    "import torch\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from flatland.envs.rail_env import RailEnv\n",
    "from flatland.envs.rail_generators import sparse_rail_generator\n",
    "from flatland.envs.malfunction_generators import ParamMalfunctionGen, MalfunctionParameters\n",
    "from flatland.envs.rail_generators import sparse_rail_generator\n",
    "from flatland.envs.line_generators import sparse_line_generator\n",
    "\n",
    "\n",
    "from utils.render import render_env\n",
    "from utils.seeding import seed_everything\n",
    "from utils.persister import load_env_from_pickle\n",
    "from utils.logger import WandbLogger\n",
    "from utils.recorder import RecorderWrapper\n",
    "from utils.levels_generator import generate_levels\n",
    "from network.rail_tranformer import RailTranformer\n",
    "# from reinforcement_learning.ppo import PPO\n",
    "from reinforcement_learning.actor_critic import ActorCritic\n",
    "from env_wrapper.railenv_wrapper import RailEnvWrapper\n",
    "from flatland_starter_kit.fast_tree_obs import FastTreeObs\n",
    "# from stable_baselines3 import PPO\n",
    "from reinforcement_learning.ppo import PPO\n",
    "from env_wrapper.skip_no_choice_wrapper import SkipNoChoiceWrapper\n",
    "\n",
    "import yappi\n",
    "\n",
    "\n",
    "### OBSERVATION ###\n",
    "TREE_OBS_DEPTH = 3  # TODO: test with higher\n",
    "obs_builder = FastTreeObs(max_depth=TREE_OBS_DEPTH)\n",
    "\n",
    "### CONFIGURATION ###\n",
    "TOT_TIMESTEPS = 2**18    #2**21  # approx 2M\n",
    "ITER_TIMESTEPS = 2**8    #2**10  # approx 1K\n",
    "NUM_ITERATIONS = TOT_TIMESTEPS // ITER_TIMESTEPS\n",
    "\n",
    "CONFIG = {\n",
    "    # Environment\n",
    "    \"test_id\": \"demo_env\",\n",
    "    \"env_id\": \"Level_1\",\n",
    "    \"skip_no_choice_steps\": False,  # TODO: reintroduci\n",
    "\n",
    "    # Observation\n",
    "    \"tree_obs_depth\": TREE_OBS_DEPTH,\n",
    "\n",
    "    # Timesteps and iterations\n",
    "    \"tot_timesteps\": TOT_TIMESTEPS,\n",
    "    \"iteration_timesteps\": ITER_TIMESTEPS,\n",
    "    \"num_iterations\": NUM_ITERATIONS,\n",
    "\n",
    "    # Network architecture\n",
    "    \"model\": \"RailTransformer\",  # \"RailTransformer\" or \"MLP\"   # TODO: implement MLP baseline or remove\n",
    "    \"state_size\": obs_builder.observation_dim,\n",
    "    \"action_size\": 4,\n",
    "    \"hidden_size\": 256,\n",
    "    \"num_layers\": 4,\n",
    "\n",
    "    # Training params\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 128,  # 2**7\n",
    "    \"learning_rate\": 2.5e-4,\n",
    "    \"kl_limit\": 0.02,\n",
    "    \"adam_eps\": 1e-5,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\n",
    "    # PPO params\n",
    "    \"gamma\": 0.999,\n",
    "    \"lambda_\": 0.95,\n",
    "    \"eps_clip\": 0.2,\n",
    "    \"entropy_bonus\": 1e-5,\n",
    "    \"v_target\": \"TD-lambda\",  # \"TD-lambda\" (for advantage + value) or \"MC\" (for cumulative reward)\n",
    "    \"normalize_v_targets\": True,    # TODO: prova con questo OFF\n",
    "\n",
    "    # Logging\n",
    "    \"log_frequency\": 10,\n",
    "    \"log_video\": False,\n",
    "    \"episode_video_frequency\": 10,\n",
    "\n",
    "    # Wandb\n",
    "    \"wandb\": True,\n",
    "\n",
    "    # Yappi profiling\n",
    "    \"profiling\": False,\n",
    "}\n",
    "\n",
    "### ENVIRONMENT ###\n",
    "pickle_train_env_path = f\"./envs_config/train_envs/{CONFIG['test_id']}/{CONFIG['env_id']}.pkl\"\n",
    "\n",
    "# generate the level if the pickle file does not exist\n",
    "if not os.path.exists(pickle_train_env_path):\n",
    "    generate_levels(\"train\", CONFIG[\"test_id\"], CONFIG[\"env_id\"])\n",
    "\n",
    "env = load_env_from_pickle(pickle_train_env_path)\n",
    "\n",
    "env.obs_builder = obs_builder\n",
    "env.obs_builder.set_env(env)\n",
    "\n",
    "# set random seed in the config\n",
    "CONFIG[\"seed\"] = env.random_seed\n",
    "\n",
    "env_size = CONFIG[\"test_id\"].split(\"_\")[0]\n",
    "\n",
    "\n",
    "### WANDB ###\n",
    "if CONFIG[\"wandb\"]:\n",
    "    wandb.login(key=\"14a7d0e7554bbddd13ca1a8d45472f7a95e73ca4\")\n",
    "    wandb.init(project=\"flatland-marl\", name=f\"{env_size}_{env.number_of_agents}\", config=CONFIG, sync_tensorboard=True)\n",
    "    config = wandb.config\n",
    "\n",
    "    wandb.define_metric(\"play/step\")\n",
    "    wandb.define_metric(\"train/batch\")\n",
    "\n",
    "    wandb.define_metric(\"play/episodic_reward\", step_metric=\"play/step\")\n",
    "    wandb.define_metric(\"play/episode_length\", step_metric=\"play/step\")\n",
    "    wandb.define_metric(\"train/loss_pi\", step_metric=\"train/batch\")\n",
    "    wandb.define_metric(\"train/loss_v\", step_metric=\"train/batch\")\n",
    "    wandb.define_metric(\"train/entropy\", step_metric=\"train/batch\")\n",
    "    wandb.define_metric(\"train/lr_policy\", step_metric=\"train/batch\")\n",
    "    wandb.define_metric(\"train/lr_value\", step_metric=\"train/batch\")\n",
    "    wandb.define_metric(\"test/episodic_reward\", step_metric=\"play/step\")\n",
    "    wandb.define_metric(\"test/episode_length\", step_metric=\"play/step\")\n",
    "else:\n",
    "    config = SimpleNamespace(**CONFIG)\n",
    "\n",
    "seed_everything(config.seed)\n",
    "\n",
    "# IMPORTANT: env must be wrapped in RailEnvWrapper before any other wrapper\n",
    "env = RailEnvWrapper(env)\n",
    "\n",
    "if config.skip_no_choice_steps:\n",
    "    env = SkipNoChoiceWrapper(env)\n",
    "\n",
    "if config.log_video:\n",
    "    env = RecorderWrapper(env, config.episode_video_frequency)\n",
    "\n",
    "# env_steps = 1000  # 2 * env.width * env.height  # Code uses 1.5 to calculate max_steps\n",
    "# rollout_fragment_length = 50\n",
    "# # env = ss.black_death_v2(env)    \n",
    "# env = ss.vector.markov_vector_wrapper.MarkovVectorEnv(env, black_death=True)    # to handle varying number of agents\n",
    "# env = ss.concat_vec_envs_v0(env, 4, num_cpus=1, base_class='stable_baselines3')\n",
    "\n",
    "# env.reset()\n",
    "# o, r, d, i = env.step({i: 0 for i in range(50)})\n",
    "# print(f\"obs: {o}\\n rewards: {r}\\n dones: {d}\\n infos: {i}\")\n",
    "# exit()\n",
    "\n",
    "### NETWORK ###\n",
    "if config.model == \"RailTransformer\":\n",
    "    policy_network = RailTranformer(config.state_size, config.action_size, config.hidden_size, config.num_layers, activation=nn.Tanh)\n",
    "    value_network = RailTranformer(config.state_size, 1, config.hidden_size, config.num_layers, activation=nn.Tanh)\n",
    "    # TODO: voglio provare sia con Tanh che con ReLU, sono troppo curiosooo\n",
    "elif config.model == \"MLP\":\n",
    "    policy_network = nn.Sequential(\n",
    "        nn.Linear(config.state_size, config.hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(config.hidden_size, config.hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(config.hidden_size, config.action_size),\n",
    "        nn.Tanh()\n",
    "    )\n",
    "    value_network = nn.Sequential(\n",
    "        nn.Linear(config.state_size, config.hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(config.hidden_size, config.hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(config.hidden_size, 1)\n",
    "    )\n",
    "\n",
    "### MODEL ###\n",
    "actor_critic = ActorCritic(policy_network, value_network, config)\n",
    "\n",
    "print(f\"Device: {config.device}\")\n",
    "actor_critic.to(config.device)\n",
    "\n",
    "optimizer = torch.optim.Adam(actor_critic.parameters(), lr=config.learning_rate, eps=config.adam_eps)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.num_iterations*config.epochs, eta_min=1e-6)\n",
    "\n",
    "\n",
    "ppo = PPO(actor_critic, env, config, optimizer, scheduler)\n",
    "\n",
    "ppo.learn()\n",
    "\n",
    "now = datetime.today().strftime('%Y%m%d-%H%M')\n",
    "ppo.save(f\"{now}_policy_flatland_{env_size}_{env.number_of_agents}_{config.tot_timesteps}_{config.seed}.pt\")\n",
    "\n",
    "# model = PPO(MlpPolicy, \n",
    "#             env, \n",
    "#             learning_rate=config.lr_policy_network, \n",
    "#             n_steps=config.iteration_timesteps,\n",
    "#             batch_size=config.batch_size, \n",
    "#             n_epochs=config.epochs, \n",
    "#             gamma=config.gamma, \n",
    "#             gae_lambda=config.lambda_,\n",
    "#             clip_range=config.eps_clip, \n",
    "#             normalize_advantage=True, \n",
    "#             ent_coef=config.entropy_bonus,\n",
    "#             # max_grad_norm=0.9, # default=0.5\n",
    "#             verbose=3, \n",
    "#             seed=config.seed)\n",
    "\n",
    "# logger = WandbLogger()\n",
    "# model.set_logger(logger=logger)\n",
    "\n",
    "# TODO: try wandb code below, I think for histograms\n",
    "# wandb.watch(model.policy.action_net, log='all', log_freq = 1)\n",
    "# wandb.watch(model.policy.value_net, log='all', log_freq = 1)\n",
    "# collect rollouts AND train on them\n",
    "\n",
    "# validate performance\n",
    "# TODO: vedi Procgen's test/eval function\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flatland-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
